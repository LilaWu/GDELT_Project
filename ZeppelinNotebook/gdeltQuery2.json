{"paragraphs":[{"text":"%md\n## Query 2\nHere we will try to resolve the following query:\n\npour un pays donné en paramètre, affichez les évènements qui y ont eu place triées par le nombre de mentions (tri décroissant); permettez une agrégation par jour/mois/année\n","user":"anonymous","dateUpdated":"2020-01-22T18:01:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Query 2</h2>\n<p>Here we will try to resolve the following query:</p>\n<p>pour un pays donné en paramètre, affichez les évènements qui y ont eu place triées par le nombre de mentions (tri décroissant); permettez une agrégation par jour/mois/année</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579715968259_1532202773","id":"20181212-102323_67420128","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:22286","dateFinished":"2020-01-22T18:01:46+0000","dateStarted":"2020-01-22T18:01:44+0000"},{"text":"%md Edit the interpreter spark (top right drop down menu) and add those two variables:\n```\nspark.jars.packages                         datastax:spark-cassandra-connector:2.4.0-s_2.11\nspark.cassandra.connection.host             private-ip-cassandra-node-1,private-ip-cassandra-node-2 \n```","user":"anonymous","dateUpdated":"2020-01-22T18:01:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Edit the interpreter spark (top right drop down menu) and add those two variables:</p>\n<pre><code>spark.jars.packages                         datastax:spark-cassandra-connector:2.4.0-s_2.11\nspark.cassandra.connection.host             private-ip-cassandra-node-1,private-ip-cassandra-node-2 \n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1579715968266_591591579","id":"20200119-122507_1982817269","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22287","dateFinished":"2020-01-22T18:01:46+0000","dateStarted":"2020-01-22T18:01:46+0000"},{"text":"val AWS_ID = \"TODO\"\nval AWS_KEY = \"TODO\"\nval AWS_TOKEN = \"TODO\"\nval s3_name = \"TODO\"\n\n\n\nsc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN)","user":"anonymous","dateUpdated":"2020-01-22T18:10:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"AWS_ID: String = ASIATATIKUDEAVI4HPCM\nAWS_KEY: String = kD5NoAvMM0E0XY4Hh1M1IYYP18efCjkCHpOrAuP8\nAWS_TOKEN: String = FwoGZXIvYXdzEKj//////////wEaDEm1Xy4uKPVuc7+F7yLMAbSC6v77M4PfmGEpgnjutvkxyyIHmzQlPP72kY5gaJ488eJsxXkuBe1oXJfVM8/uujskeSrV6E4fzV9MGVFEPnWdqyP3QiINyDVPAw/mZpqMsDEoc2qzXUf7/yH4nA6dTQXll1LJu0lGFU9elHvDAgMOb+vfKnmZC/jpifxMGznk1VhMJYyfwiyHIQz3LSy6sN16A1BZlnNb3nfnXk0UejD96dB2KL5ytNH3y/HYP+kKIxxTfcASt3IWSNkQefQcTTsPCtfPxPsIMukKwSj9tqHxBTIt0Ku7GVHP55pC3q7WtcTPtTo7eeT01g1TKiiA3tvRpGSECO5TWDhBy8FilTFp\ns3_name: String = gael-sav-telecom-gdelt2019-new\n"}]},"apps":[],"jobName":"paragraph_1579715968266_-107829345","id":"20171217-230735_1688540039","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22288","dateFinished":"2020-01-22T18:01:47+0000","dateStarted":"2020-01-22T18:01:46+0000"},{"text":"","user":"anonymous","dateUpdated":"2020-01-22T18:01:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579715968267_1868962677","id":"20200122-163503_787379322","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22289"},{"text":"%md Exportation of the Tables Events and Mentions","user":"anonymous","dateUpdated":"2020-01-22T18:01:47+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Exportation of the Tables Events and Mentions</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579715968267_-387701137","id":"20200117-103749_1442620581","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22290","dateFinished":"2020-01-22T18:01:47+0000","dateStarted":"2020-01-22T18:01:47+0000"},{"text":"import org.apache.spark.{SparkConf, SparkContext, sql}\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.rdd.RDD\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\nimport java.io.File\n//Cassandra\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\n\n\n    val textRDDEvents: RDD[String] = sc.binaryFiles(\"s3://\" + s3_name + \"/2019120105**.export.CSV.zip\").\n      flatMap { // decompresser les fichiers\n        case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n            takeWhile(_ != null).\n            flatMap { _ =>\n              val br = new BufferedReader(new InputStreamReader(zis))\n              Stream.continually(br.readLine()).takeWhile(_ != null)\n            }\n      }\n\n    // *** Mentions ***\n    val textRDDMentions: RDD[String] = sc.binaryFiles(\"s3://\" + s3_name + \"/2019120105*.mentions.CSV.zip\").\n      flatMap { // decompresser les fichiers\n        case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n            takeWhile(_ != null).\n            flatMap { _ =>\n              val br = new BufferedReader(new InputStreamReader(zis))\n              Stream.continually(br.readLine()).takeWhile(_ != null)\n            }\n      }\n\n    /** ************************************************************\n     * Ajout des informations colonnes et creation de deux Dataframe\n     * ************************************************************* */\n\n    // EVENTS\n\n    val dfEvents: DataFrame = textRDDEvents.toDF.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n      .withColumn(\"Day\", split($\"value\", \"\\\\t\").getItem(1))\n      .withColumn(\"MonthYear\", split($\"value\", \"\\\\t\").getItem(2))\n      .withColumn(\"Year\", split($\"value\", \"\\\\t\").getItem(3))\n      .withColumn(\"FractionDate\", split($\"value\", \"\\\\t\").getItem(4))\n      .withColumn(\"Actor1Code\", split($\"value\", \"\\\\t\").getItem(5))\n      .withColumn(\"Actor1Name\", split($\"value\", \"\\\\t\").getItem(6))\n      .withColumn(\"Actor1CountryCode\", split($\"value\", \"\\\\t\").getItem(7))\n      .withColumn(\"Actor1KnownGroupCode\", split($\"value\", \"\\\\t\").getItem(8))\n      .withColumn(\"Actor1EthnicCode\", split($\"value\", \"\\\\t\").getItem(9))\n      .withColumn(\"Actor1Religion1Code\", split($\"value\", \"\\\\t\").getItem(10))\n      .withColumn(\"Actor1Religion2Code\", split($\"value\", \"\\\\t\").getItem(11))\n      .withColumn(\"Actor1Type1Code\", split($\"value\", \"\\\\t\").getItem(12))\n      .withColumn(\"Actor1Type2Code\", split($\"value\", \"\\\\t\").getItem(13))\n      .withColumn(\"Actor1Type3Code\", split($\"value\", \"\\\\t\").getItem(14))\n      .withColumn(\"Actor2Code\", split($\"value\", \"\\\\t\").getItem(15))\n      .withColumn(\"Actor2Name\", split($\"value\", \"\\\\t\").getItem(16))\n      .withColumn(\"Actor2CountryCode\", split($\"value\", \"\\\\t\").getItem(17))\n      .withColumn(\"Actor2KnownGroupCode\", split($\"value\", \"\\\\t\").getItem(18))\n      .withColumn(\"Actor2EthnicCode\", split($\"value\", \"\\\\t\").getItem(19))\n      .withColumn(\"Actor2Religion1Code\", split($\"value\", \"\\\\t\").getItem(20))\n      .withColumn(\"Actor2Religion2Code\", split($\"value\", \"\\\\t\").getItem(21))\n      .withColumn(\"Actor2Type1Code\", split($\"value\", \"\\\\t\").getItem(22))\n      .withColumn(\"Actor2Type2Code\", split($\"value\", \"\\\\t\").getItem(23))\n      .withColumn(\"Actor2Type3Code\", split($\"value\", \"\\\\t\").getItem(24))\n      .withColumn(\"IsRootEvent\", split($\"value\", \"\\\\t\").getItem(25))\n      .withColumn(\"EventCode\", split($\"value\", \"\\\\t\").getItem(26))\n      .withColumn(\"EventBaseCode\", split($\"value\", \"\\\\t\").getItem(27))\n      .withColumn(\"EventRootCode\", split($\"value\", \"\\\\t\").getItem(28))\n      .withColumn(\"QuadClass\", split($\"value\", \"\\\\t\").getItem(29))\n      .withColumn(\"GoldsteinScale\", split($\"value\", \"\\\\t\").getItem(30))\n      .withColumn(\"NumMentions\", split($\"value\", \"\\\\t\").getItem(31))\n      .withColumn(\"NumSources\", split($\"value\", \"\\\\t\").getItem(32))\n      .withColumn(\"NumArticles\", split($\"value\", \"\\\\t\").getItem(33))\n      .withColumn(\"AvgTone\", split($\"value\", \"\\\\t\").getItem(34))\n      .withColumn(\"Actor1Geo_Type\", split($\"value\", \"\\\\t\").getItem(35))\n      .withColumn(\"Actor1Geo_FullName\", split($\"value\", \"\\\\t\").getItem(36))\n      .withColumn(\"Actor1Geo_CountryCode\", split($\"value\", \"\\\\t\").getItem(37))\n      .withColumn(\"Actor1Geo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(38))\n      .withColumn(\"Actor1Geo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(39))\n      .withColumn(\"Actor1Geo_Lat\", split($\"value\", \"\\\\t\").getItem(40))\n      .withColumn(\"Actor1Geo_Long\", split($\"value\", \"\\\\t\").getItem(41))\n      .withColumn(\"Actor1Geo_FeatureID\", split($\"value\", \"\\\\t\").getItem(42))\n      .withColumn(\"Actor2Geo_Type\", split($\"value\", \"\\\\t\").getItem(43))\n      .withColumn(\"Actor2Geo_FullName\", split($\"value\", \"\\\\t\").getItem(44))\n      .withColumn(\"Actor2Geo_CountryCode\", split($\"value\", \"\\\\t\").getItem(45))\n      .withColumn(\"Actor2Geo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(46))\n      .withColumn(\"Actor2Geo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(47))\n      .withColumn(\"Actor2Geo_Lat\", split($\"value\", \"\\\\t\").getItem(48))\n      .withColumn(\"Actor2Geo_Long\", split($\"value\", \"\\\\t\").getItem(49))\n      .withColumn(\"Actor2Geo_FeatureID\", split($\"value\", \"\\\\t\").getItem(50))\n      .withColumn(\"ActionGeo_Type\", split($\"value\", \"\\\\t\").getItem(51))\n      .withColumn(\"ActionGeo_FullName\", split($\"value\", \"\\\\t\").getItem(52))\n      .withColumn(\"ActionGeo_CountryCode\", split($\"value\", \"\\\\t\").getItem(53))\n      .withColumn(\"ActionGeo_ADM1Code\", split($\"value\", \"\\\\t\").getItem(54))\n      .withColumn(\"ActionGeo_ADM2Code\", split($\"value\", \"\\\\t\").getItem(55))\n      .withColumn(\"ActionGeo_Lat\", split($\"value\", \"\\\\t\").getItem(56))\n      .withColumn(\"ActionGeo_Long\", split($\"value\", \"\\\\t\").getItem(57))\n      .withColumn(\"ActionGeo_FeatureID\", split($\"value\", \"\\\\t\").getItem(58))\n      .withColumn(\"DATEADDED\", split($\"value\", \"\\\\t\").getItem(59))\n      .withColumn(\"SOURCEURL\", split($\"value\", \"\\\\t\").getItem(60))\n      .drop(\"value\")\n\n    // MENTIONS\n    val dfMentions: DataFrame = textRDDMentions.toDF.withColumn(\"GLOBALEVENTID\", split($\"value\", \"\\\\t\").getItem(0))\n      .withColumn(\"EventTimeDate\", split($\"value\", \"\\\\t\").getItem(1))\n      .withColumn(\"MentionTimeDate\", split($\"value\", \"\\\\t\").getItem(2))\n      .withColumn(\"MentionType\", split($\"value\", \"\\\\t\").getItem(3))\n      .withColumn(\"MentionSourceName\", split($\"value\", \"\\\\t\").getItem(4))\n      .withColumn(\"MentionIdentifier\", split($\"value\", \"\\\\t\").getItem(5))\n      .withColumn(\"SentenceID\", split($\"value\", \"\\\\t\").getItem(6))\n      .withColumn(\"Actor1CharOffset\", split($\"value\", \"\\\\t\").getItem(7))\n      .withColumn(\"Actor2CharOffset\", split($\"value\", \"\\\\t\").getItem(8))\n      .withColumn(\"ActionCharOffset\", split($\"value\", \"\\\\t\").getItem(9))\n      .withColumn(\"InRawText\", split($\"value\", \"\\\\t\").getItem(10))\n      .withColumn(\"Confidence\", split($\"value\", \"\\\\t\").getItem(11))\n      .withColumn(\"MentionDocLen\", split($\"value\", \"\\\\t\").getItem(12))\n      .withColumn(\"MentionDocTone\", split($\"value\", \"\\\\t\").getItem(13))\n      .withColumn(\"MentionDocTranslationInfo\", split($\"value\", \"\\\\t\").getItem(14))\n      .withColumn(\"Extras\", split($\"value\", \"\\\\t\").getItem(15))\n      .drop(\"value\")\n      \n    val dfJoin = dfEvents.join(dfMentions, Seq(\"GLOBALEVENTID\"),\"left\")","user":"anonymous","dateUpdated":"2020-01-22T18:01:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.{SparkConf, SparkContext, sql}\nimport org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport org.apache.spark.rdd.RDD\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicSessionCredentials\nimport java.io.File\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport org.apache.spark.sql.cassandra._\ntextRDDEvents: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[252] at flatMap at <console>:250\ntextRDDMentions: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[254] at flatMap at <console>:263\ndfEvents: org.apache.spark.sql.DataFram..."}]},"apps":[],"jobName":"paragraph_1579715968267_-1795619218","id":"20200117-103556_2139418060","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22291","dateFinished":"2020-01-22T18:01:51+0000","dateStarted":"2020-01-22T18:01:47+0000"},{"text":"    val dfPays_Events = dfJoin.select(\"GLOBALEVENTID\",\"ActionGeo_CountryCode\", \"Day\")\n      .groupBy(\"ActionGeo_CountryCode\",\"Day\",\"GLOBALEVENTID\")\n      .agg(count($\"GLOBALEVENTID\").alias(\"num_mentions\"))\n      .orderBy($\"num_mentions\".desc)\n\n    dfPays_Events.show(25)","user":"anonymous","dateUpdated":"2020-01-22T18:01:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------------+--------+-------------+------------+\n|ActionGeo_CountryCode|     Day|GLOBALEVENTID|num_mentions|\n+---------------------+--------+-------------+------------+\n|                   UK|20191201|    890046028|          38|\n|                   UK|20191201|    890046029|          38|\n|                   BL|20191201|    890046810|           9|\n|                   TN|20191124|    890045803|           9|\n|                   GM|20191124|    890045804|           9|\n|                   MX|20191201|    890047218|           7|\n|                   MX|20191201|    890047219|           7|\n|                   MX|20191201|    890046957|           7|\n|                   MX|20191201|    890046958|           7|\n|                   JM|20191201|    890047144|           7|\n|                   BL|20191201|    890046807|           6|\n|                   BL|20191201|    890046927|           5|\n|                   BL|20191201|    890047073|           5|\n|                   MX|20191201|    890049067|           5|\n|                   MX|20191201|    890049088|           5|\n|                   BL|20191201|    890047065|           5|\n|                   BL|20191201|    890046806|           5|\n|                   BL|20191201|    890047107|           5|\n|                   BL|20191201|    890046811|           5|\n|                   BL|20191201|    890046952|           5|\n|                   BL|20191201|    890046808|           5|\n|                   BL|20191201|    890047084|           5|\n|                   BL|20191201|    890046809|           5|\n|                   MX|20181201|    890047756|           4|\n|                   HK|20191201|    890045949|           4|\n+---------------------+--------+-------------+------------+\nonly showing top 25 rows\n\ndfPays_Events: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ActionGeo_CountryCode: string, Day: string ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579715968267_-1077847777","id":"20200120-145114_877903678","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22292","dateFinished":"2020-01-22T18:02:09+0000","dateStarted":"2020-01-22T18:01:51+0000"},{"text":"    val dfPays_Events = dfJoin.select(\"GLOBALEVENTID\",\"ActionGeo_CountryCode\", \"Day\")\n      .withColumn(\"year\", substring($\"Day\", 0, 4))\n      .withColumn(\"month\", substring($\"Day\", 5, 2))\n      .withColumn(\"day\", substring($\"Day\", 7, 2))\n      .groupBy(\"ActionGeo_CountryCode\",\"GLOBALEVENTID\",\"year\", \"month\", \"day\")\n      .agg(count($\"GLOBALEVENTID\").alias(\"num_mentions\"))\n      .orderBy($\"num_mentions\".desc)\n     \n     dfPays_Events.show(25)","user":"anonymous","dateUpdated":"2020-01-22T18:02:09+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------------------+-------------+----+-----+---+------------+\n|ActionGeo_CountryCode|GLOBALEVENTID|year|month|day|num_mentions|\n+---------------------+-------------+----+-----+---+------------+\n|                   UK|    890046029|2019|   12| 01|          38|\n|                   UK|    890046028|2019|   12| 01|          38|\n|                   BL|    890046810|2019|   12| 01|           9|\n|                   TN|    890045803|2019|   11| 24|           9|\n|                   GM|    890045804|2019|   11| 24|           9|\n|                   MX|    890046957|2019|   12| 01|           7|\n|                   MX|    890047218|2019|   12| 01|           7|\n|                   MX|    890047219|2019|   12| 01|           7|\n|                   JM|    890047144|2019|   12| 01|           7|\n|                   MX|    890046958|2019|   12| 01|           7|\n|                   BL|    890046807|2019|   12| 01|           6|\n|                   MX|    890049067|2019|   12| 01|           5|\n|                   BL|    890046806|2019|   12| 01|           5|\n|                   BL|    890047107|2019|   12| 01|           5|\n|                   BL|    890046952|2019|   12| 01|           5|\n|                   BL|    890046809|2019|   12| 01|           5|\n|                   BL|    890047065|2019|   12| 01|           5|\n|                   BL|    890046808|2019|   12| 01|           5|\n|                   BL|    890046927|2019|   12| 01|           5|\n|                   MX|    890049088|2019|   12| 01|           5|\n|                   BL|    890046811|2019|   12| 01|           5|\n|                   BL|    890047073|2019|   12| 01|           5|\n|                   BL|    890047084|2019|   12| 01|           5|\n|                   HK|    890045949|2019|   12| 01|           4|\n|                   US|    890048566|2019|   12| 01|           4|\n+---------------------+-------------+----+-----+---+------------+\nonly showing top 25 rows\n\ndfPays_Events: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ActionGeo_CountryCode: string, GLOBALEVENTID: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579715968267_1974320087","id":"20200122-151557_1100523327","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22293","dateFinished":"2020-01-22T18:02:19+0000","dateStarted":"2020-01-22T18:02:09+0000"},{"text":"\n    val df_country_events = dfEvents.select(\"GLOBALEVENTID\",\"ActionGeo_CountryCode\", \"Day\")\n      .filter(!($\"ActionGeo_CountryCode\".isNaN || $\"ActionGeo_CountryCode\".isNull || $\"ActionGeo_CountryCode\" === \"\"))\n      .join(\n        dfMentions.select(\"GLOBALEVENTID\"), \"GLOBALEVENTID\")\n      .withColumn(\"year\", substring($\"Day\", 0, 4))\n      .withColumn(\"month\", substring($\"Day\", 5, 2))\n      .withColumn(\"day\", substring($\"Day\", 7, 2))\n      .groupBy(\"ActionGeo_CountryCode\",\"GLOBALEVENTID\",\"year\", \"month\", \"day\")\n      .agg(count($\"GLOBALEVENTID\").alias(\"num_mentions\"))\n      .orderBy($\"num_mentions\".desc)\n      .withColumnRenamed(\"ActionGeo_CountryCode\",\"country\")\n      .withColumnRenamed(\"GLOBALEVENTID\",\"event\")\n      \n    df_country_events.show(10)","user":"anonymous","dateUpdated":"2020-01-22T18:04:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------+---------+----+-----+---+------------+\n|country|    event|year|month|day|num_mentions|\n+-------+---------+----+-----+---+------------+\n|     UK|890046028|2019|   12| 01|          38|\n|     UK|890046029|2019|   12| 01|          38|\n|     TN|890045803|2019|   11| 24|           9|\n|     BL|890046810|2019|   12| 01|           9|\n|     GM|890045804|2019|   11| 24|           9|\n|     MX|890046957|2019|   12| 01|           7|\n|     JM|890047144|2019|   12| 01|           7|\n|     MX|890047218|2019|   12| 01|           7|\n|     MX|890047219|2019|   12| 01|           7|\n|     MX|890046958|2019|   12| 01|           7|\n+-------+---------+----+-----+---+------------+\nonly showing top 10 rows\n\ndf_country_events: org.apache.spark.sql.DataFrame = [country: string, event: string ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579715968268_-296918766","id":"20200122-150533_419453088","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22294","dateFinished":"2020-01-22T18:05:50+0000","dateStarted":"2020-01-22T18:04:34+0000"},{"text":"%md Processing of the data","user":"anonymous","dateUpdated":"2020-01-22T18:02:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Processing of the data</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579715968268_1242512387","id":"20200119-122641_344541628","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22295","dateFinished":"2020-01-22T18:02:28+0000","dateStarted":"2020-01-22T18:02:28+0000"},{"text":"    import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}\n\n    df_country_events\n      .write\n      .mode(SaveMode.Overwrite)\n      .parquet(\"s3://\" + s3_name + \"/df_country_events.parquet/\")","user":"anonymous","dateUpdated":"2020-01-22T18:05:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":325,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n  ... 122 elided\nCaused by: org.apache.spark.SparkException: Job 49 cancelled part of cancelled job group zeppelin-anonymous-2EXZSKKZN-20200117-102214_1513863359\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1976)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:946)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:946)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:946)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:946)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2231)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n  at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:224)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:41)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n  ... 144 more\n"}]},"apps":[],"jobName":"paragraph_1579715968268_1845068312","id":"20200117-102214_1513863359","dateCreated":"2020-01-22T17:59:28+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:22296","dateFinished":"2020-01-22T18:07:20+0000","dateStarted":"2020-01-22T18:05:57+0000"},{"text":"%md Creation of Cassandra table ","user":"anonymous","dateUpdated":"2020-01-22T18:02:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Creation of Cassandra table</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579715968268_792454470","id":"20200119-122701_546900147","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22297","dateFinished":"2020-01-22T18:02:37+0000","dateStarted":"2020-01-22T18:02:37+0000"},{"text":"CassandraConnector(sc.getConf).withSessionDo { session =>\n      session.execute(\n        \"\"\"\n           CREATE KEYSPACE IF NOT EXISTS gdelt\n           WITH REPLICATION =\n           {'class': 'SimpleStrategy', 'replication_factor': 2 };\n        \"\"\")\n      session.execute(\n        \"\"\"\n           CREATE TABLE IF NOT EXISTS gdelt.country_events (\n              event int,\n              country text,\n              year int,\n              month int,\n              day int,\n              num_mentions int,\n              PRIMARY KEY (event, year, month, day, num_mentions)\n            );\n        \"\"\"\n      )\n}","user":"anonymous","dateUpdated":"2020-01-22T18:07:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res57: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n"}]},"apps":[],"jobName":"paragraph_1579715968269_-497788799","id":"20200122-162256_468552736","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22298","dateFinished":"2020-01-22T18:07:26+0000","dateStarted":"2020-01-22T18:07:24+0000"},{"text":"    df_country_events.write\n      .cassandraFormat(\"country_events\", \"gdelt\")\n      .save()","user":"anonymous","dateUpdated":"2020-01-22T18:07:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579715968269_859589944","id":"20200122-162447_1700300822","dateCreated":"2020-01-22T17:59:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:22299","dateFinished":"2020-01-22T18:08:56+0000","dateStarted":"2020-01-22T18:07:28+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579716551508_515604092","id":"20200122-180911_1179998160","dateCreated":"2020-01-22T18:09:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:24006","text":"val query2 = spark.read\n      .cassandraFormat(\"country_events\", \"gdelt\")\n      .load()","dateUpdated":"2020-01-22T18:09:30+0000","dateFinished":"2020-01-22T18:09:31+0000","dateStarted":"2020-01-22T18:09:30+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"query2: org.apache.spark.sql.DataFrame = [event: int, year: int ... 4 more fields]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579716562874_-723670728","id":"20200122-180922_1988734698","dateCreated":"2020-01-22T18:09:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:24078","text":"query2.show(20)","dateUpdated":"2020-01-22T18:09:44+0000","dateFinished":"2020-01-22T18:09:45+0000","dateStarted":"2020-01-22T18:09:44+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+----+-----+---+------------+-------+\n|    event|year|month|day|num_mentions|country|\n+---------+----+-----+---+------------+-------+\n|890047741|2019|   12|  1|           1|     KZ|\n|890048616|2019|   12|  1|           1|     UP|\n|890051456|2019|   12|  1|           1|     LE|\n|890051429|2019|   12|  1|           1|     GM|\n|890048539|2019|   12|  1|           1|     UP|\n|890048849|2019|   12|  1|           1|     US|\n|890047585|2019|   12|  1|           1|     IZ|\n|890046753|2019|   12|  1|           1|     AF|\n|890047347|2019|   12|  1|           1|     US|\n|890048235|2019|   12|  1|           1|     SY|\n|890047017|2019|   12|  1|           1|     BG|\n|890048316|2019|   12|  1|           1|     US|\n|890046906|2019|   12|  1|           1|     US|\n|890047207|2019|   12|  1|           1|     SN|\n|890051489|2019|   12|  1|           1|     FR|\n|890050459|2019|   12|  1|           1|     SP|\n|890048622|2019|   12|  1|           1|     UP|\n|890050376|2019|   12|  1|           1|     RS|\n|890047501|2019|   12|  1|           1|     AR|\n|890049661|2019|   12|  1|           1|     US|\n+---------+----+-----+---+------------+-------+\nonly showing top 20 rows\n\n"}]}},{"text":"%md Import of the dataframe data in Cassandra","user":"anonymous","dateUpdated":"2020-01-22T17:59:28+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579715968269_-211337342","id":"20200119-122733_569981743","dateCreated":"2020-01-22T17:59:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22300"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-01-22T17:59:28+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579715968269_-1201675939","id":"20200122-160058_898168081","dateCreated":"2020-01-22T17:59:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22301"}],"name":"gdeltQuery2","id":"2EXZSKKZN","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"angular:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}