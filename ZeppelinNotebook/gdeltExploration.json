{"paragraphs":[{"text":"%md\n## Exploration des donnees GDELT via Spark\nDans ce notebook nous allons commencer a explorer les donnees GDELT qu'on a stoque sur S3","user":"anonymous","dateUpdated":"2020-01-09T16:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Exploration des donnees GDELT via Spark</h2>\n<p>Dans ce notebook nous allons commencer a explorer les donnees GDELT qu&rsquo;on a stoque sur S3</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578587184369_1469733188","id":"20181212-102323_67420128","dateCreated":"2020-01-09T16:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:194"},{"text":"val AWS_ID = \"TODO\"\nval AWS_KEY = \"TODO\"\nval AWS_TOKEN = \"TODO\"\n\nsc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN)","user":"anonymous","dateUpdated":"2020-01-09T18:49:05+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"AWS_ID: String = ASIAX7FDIQZH762DZAEB\nAWS_KEY: String = nmssmdL1gM4pZxt2SRfKuHipql0F21sEjgjZdDqj\nAWS_TOKEN: String = FwoGZXIvYXdzEHIaDMMg3HvkMXIrcVvjFyK/AYFkwAxM5n7DcbteJQYbrd/L6/6nGvr5VKJOS+3n0x8dtOzpJsn5UXKaMrQJg9IjpM+dXtZ1N2Q1laIQaYUdvdwe10IK9Ne5j1t9Wd84hrP4lHQJL8chDHG/FkPVDXfvHhcwrM7lOLRnsEg8j6euqS9Tl7rXYR/2Yywxf+/aXX8ScOib5o5bx9MRgdlVZ1o1sr1JxP9tmjm2Yp1h2r6p4sL00cF0iZPnJ1tPowREIiV2Ox4tJqE+ryAmnQPxP8E3KNq53fAFMi2DPup3DOgTxArjBm9mgBnnOgCYbYvZP9Y1VkV7VCVdIrTjXIiJEgVXj71LYpQ=\n"}]},"apps":[],"jobName":"paragraph_1578587184373_1005185702","id":"20171217-230735_1688540039","dateCreated":"2020-01-09T16:26:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:195","dateFinished":"2020-01-09T18:26:16+0000","dateStarted":"2020-01-09T18:26:15+0000"},{"text":"%md Les fichiers sont stoquees compresses, on a besoin d'un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu'on les lit depuis S3:","user":"anonymous","dateUpdated":"2020-01-09T16:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les fichiers sont stoquees compresses, on a besoin d&rsquo;un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu&rsquo;on les lit depuis S3:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578587184374_-92660745","id":"20181212-102329_808049084","dateCreated":"2020-01-09T16:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n// 20181201000000.export.CSV.zip\nval textRDD = sc.binaryFiles(\"s3://projet-gdelt-2019/20181201[0-9]*.export.CSV.zip\"). // charger quelques fichers via une regex\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile(_ != null).\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\ntextRDD.take(1)\n","user":"anonymous","dateUpdated":"2020-01-09T18:34:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Can not create a Path from an empty string\n  at org.apache.hadoop.fs.Path.checkPathArg(Path.java:163)\n  at org.apache.hadoop.fs.Path.<init>(Path.java:175)\n  at org.apache.hadoop.fs.Path.<init>(Path.java:120)\n  at org.apache.hadoop.fs.Globber.doGlob(Globber.java:258)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1760)\n  at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:408)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:300)\n  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:271)\n  at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)\n  at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n  ... 60 elided\n"}]},"apps":[],"jobName":"paragraph_1578587184374_-12598042","id":"20171217-232457_1732696781","dateCreated":"2020-01-09T16:26:24+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:197","dateFinished":"2020-01-09T18:34:43+0000","dateStarted":"2020-01-09T18:34:42+0000"},{"text":"%md A vous de jouer ! Utilisez la documentation GDELT(https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/) et commencez a explorer les donnees via les API Spark.","user":"anonymous","dateUpdated":"2020-01-09T16:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A vous de jouer ! Utilisez la documentation GDELT(<a href=\"https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/\">https://blog.gdeltproject.org/gdelt-2-0-our-global-world-in-realtime/</a>) et commencez a explorer les donnees via les API Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1578587184375_60416766","id":"20171218-084519_765381887","dateCreated":"2020-01-09T16:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:198"}],"name":"gdeltExploration","id":"2EXPHD7JC","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}